
Training the network for 10 epochs, with a batch size of 5
/share/etud/e2008984/miniconda3/envs/env_thesis/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
  0%|          | 0/4 [00:00<?, ?it/s]/share/etud/e2008984/miniconda3/envs/env_thesis/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:114: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.")



Training Epoch [0/10]:  75%|███████▌  | 3/4 [00:32<00:10, 10.64s/it, acc=0.28901562, iou=0.0019474078, loss=0.996]






























Training Epoch [5/10]:  75%|███████▌  | 3/4 [00:21<00:07,  7.41s/it, acc=0.6608826, iou=0.003852596, loss=0.993]




























Training the network for 10 epochs, with a batch size of 5
/share/etud/e2008984/miniconda3/envs/env_thesis/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
  0%|          | 0/4 [00:00<?, ?it/s]/share/etud/e2008984/miniconda3/envs/env_thesis/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:114: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.")







































Training Epoch [7/10]:  75%|███████▌  | 3/4 [00:09<00:03,  3.00s/it, acc=0.76647997, iou=0.0038282988, loss=0.995]





Training Epoch [8/10]:  75%|███████▌  | 3/4 [00:08<00:02,  2.95s/it, acc=0.7678362, iou=0.0038924182, loss=0.995]








Training the network for 2 epochs, with a batch size of 2 and 1 iterations per epoch ðŸ¤—
  0%|          | 0/2 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s]
loss tensor(0.9967, grad_fn=<MeanBackward0>)

 10%|â–ˆ         | 1/10 [00:06<00:57,  6.40s/it, acc=tensor(0.1102), iou=tensor(0.0016), loss=0.997]
loss tensor(0.9949, grad_fn=<MeanBackward0>)

 20%|â–ˆâ–ˆ        | 2/10 [00:13<00:55,  6.94s/it, acc=tensor(0.7318), iou=tensor(0.0035), loss=0.995]
loss tensor(0.9948, grad_fn=<MeanBackward0>)

 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:19<00:45,  6.51s/it, acc=tensor(0.6630), iou=tensor(0.0028), loss=0.995]
loss tensor(0.9952, grad_fn=<MeanBackward0>)

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:24<00:35,  5.87s/it, acc=tensor(0.7262), iou=tensor(0.0017), loss=0.995]
loss tensor(0.9991, grad_fn=<MeanBackward0>)

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:29<00:27,  5.47s/it, acc=tensor(0.5957), iou=tensor(0.0005), loss=0.999]
loss tensor(0.9996, grad_fn=<MeanBackward0>)


 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:38<00:14,  4.97s/it, acc=tensor(0.4859), iou=tensor(0.0020), loss=0.996]
loss tensor(0.9964, grad_fn=<MeanBackward0>)
[INFO] Training Loop
loss tensor(0.9969, grad_fn=<MeanBackward0>)

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:42<00:09,  4.84s/it, acc=tensor(0.4807), iou=tensor(0.0017), loss=0.997]
loss tensor(0.9986, grad_fn=<MeanBackward0>)


 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:47<00:04,  4.72s/it, acc=tensor(0.5355), iou=tensor(0.0009), loss=0.999]
loss tensor(0.9999, grad_fn=<MeanBackward0>)
[INFO] Training Loop
(2, 512, 512)
(2, 512, 512)
(2, 512, 512)
(2, 512, 512)

  0%|          | 0/2 [01:01<?, ?it/s]
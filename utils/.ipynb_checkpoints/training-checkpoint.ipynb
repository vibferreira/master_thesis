{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import model\n",
    "import metrics\n",
    "import config\n",
    "import utis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from dataset import HistoricalImagesDataset\n",
    "import config\n",
    "import glob\n",
    "\n",
    "# Ignore excessive warnings\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# WandB – Import the wandb library\n",
    "import wandb\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMAGES_PATH = '../data/patches/images/1942'\n",
    "MASK_PATH = '../data/patches/masks/1942'\n",
    "BEST_MODEL = '../best_model'\n",
    "# PREDICTIONS_PATH = \n",
    "\n",
    "image_paths = glob.glob(IMAGES_PATH +'/*.tif')[:100]\n",
    "mask_paths = glob.glob(MASK_PATH +'/*.tif')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 95, 117, 118, ..., 213, 212, 203],\n",
       "       [102, 120, 122, ..., 212, 212, 211],\n",
       "       [104, 117, 117, ..., 209, 212, 216],\n",
       "       ...,\n",
       "       [162, 176, 178, ..., 120, 119, 118],\n",
       "       [159, 167, 168, ..., 129, 115, 112],\n",
       "       [186, 192, 187, ..., 131, 129, 135]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(image_paths[0], cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(image_paths[0])\n",
    "mask = Image.open(mask_paths[0])\n",
    "image = np.array(image)\n",
    "mask = np.array(mask)\n",
    " \n",
    "transform = A.Compose([\n",
    "      A.Rotate(limit=40,p=0.9, border_mode=cv2.BORDER_CONSTANT), # p stands for the probability with which the transformations are applied\n",
    "      A.HorizontalFlip(p=0.5),\n",
    "      A.VerticalFlip(p=0.1), \n",
    "      A.Normalize(mean=(0), std=(1)),\n",
    "      ToTensorV2()])\n",
    "\n",
    "# list_images = []\n",
    "# for i in range(15):\n",
    "#     augmentations = transform(image=image, mask=mask)\n",
    "#     augmented_img = augmentations['image']\n",
    "#     augmented_mask = augmentations['mask']\n",
    "\n",
    "#     list_images.append(augmented_img)\n",
    "#     list_images.append(augmented_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(list_images[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvibferreira\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image patches: 100 \n",
      "Number of mask patches: 100\n",
      "shape image torch.Size([1, 256, 256]) shape mask torch.Size([256, 256])\n",
      "Splitting data into TRAIN, VAL and TEST\n",
      "Training set size:  50\n",
      "Validation set size:  25\n",
      "Testing set size:  25\n"
     ]
    }
   ],
   "source": [
    "# Dataset Object \n",
    "print('Number of image patches:', len(image_paths),'\\nNumber of mask patches:', len(mask_paths))\n",
    "dataset = HistoricalImagesDataset(image_paths, mask_paths, transform=transform)\n",
    "data = next(iter(dataset))\n",
    "print('shape image', data[0].shape, 'shape mask', data[1].shape)       \n",
    "\n",
    "# Train, Test, Split -- DEVEOLP A SPLITTING STRATEGY BASED ON THE SPATIAL INFORMATION !!!!!!!!!\n",
    "print('Splitting data into TRAIN, VAL and TEST')\n",
    "train_size = int(0.5 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) #-- pytorch alternative to the train_test_split command line from Scikit-Learn\n",
    "\n",
    "train_size = int(0.5 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "test_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "print(\"Training set size: \", len(train_dataset))\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size = config.BATCH_SIZE)\n",
    "print(\"Validation set size: \", len(val_dataset))\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size = config.BATCH_SIZE)\n",
    "print(\"Testing set size: \", len(test_dataset))\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size = config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader_iter = iter(val_dataloader)\n",
    "# grids = utis.create_segement_grids(val_dataloader_iter)\n",
    "# utis.plot_grids(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(val_dataloader_iter)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.142857142857143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_n = len(train_dataset)/len(train_dataloader)\n",
    "iter_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optim, lossFunc, epoch, scaler):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Save total train loss\n",
    "    totalTrainLoss = 0\n",
    "    \n",
    "    # metrics\n",
    "    accuracy = 0\n",
    "    iou = 0\n",
    "    f1score = 0\n",
    "    dice = 0\n",
    "    \n",
    "    # testing\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    loop = tqdm(dataloader, leave=False)\n",
    "    for x, y in loop:\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(config.DEVICE), y.float().to(config.DEVICE))\n",
    "        \n",
    "        # forward with autocast        \n",
    "        with autocast():\n",
    "            pred = model(x)\n",
    "            loss = lossFunc(pred, y)\n",
    "            \n",
    "        optim.zero_grad()  # zero out any previously accumulated gradients    \n",
    "        scaler.scale(loss).backward() # study this \n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        \n",
    "        totalTrainLoss += loss  # add the loss to the total training loss so far \n",
    "        \n",
    "        # metrics      \n",
    "        all_metrics = metrics.metrics(pred, y)\n",
    "        accuracy += all_metrics['acc']\n",
    "        iou += all_metrics['iou']\n",
    "        f1score += all_metrics['f1score']\n",
    "        dice += all_metrics['dice_coeff']\n",
    "        \n",
    "        tp += all_metrics['false_preds'][0]\n",
    "        fp += all_metrics['false_preds'][1]\n",
    "        fn += all_metrics['false_preds'][2]\n",
    "        tn += all_metrics['false_preds'][3]\n",
    "        \n",
    "        # update tqdm\n",
    "        loop.set_description(f'Training Epoch [{epoch}/{config.NUM_EPOCHS}]')\n",
    "        loop.set_postfix(loss=loss.item(), acc = all_metrics['acc'], iou=all_metrics['iou'], dice = all_metrics['dice_coeff'])\n",
    "        \n",
    "    # calculate the average training loss PER EPOCH\n",
    "    avgTrainLoss = totalTrainLoss / len(dataloader)\n",
    "    avgAccLoss = accuracy / len(dataloader)\n",
    "    avgIOU = iou / iter_n\n",
    "    avgF1score = f1score / len(dataloader)\n",
    "    avgDice = dice / len(dataloader)\n",
    "    \n",
    "    avg_tp = tp / len(dataloader)\n",
    "    avg_fp = fp / len(dataloader)\n",
    "    avg_fn = fn / len(dataloader)\n",
    "    avg_tn = tn / len(dataloader)\n",
    "    \n",
    "    ## update training history\n",
    "    training_history[\"avg_train_loss\"].append(avgTrainLoss.cpu().detach().numpy()) # save the avg loss\n",
    "    training_history[\"train_accuracy\"].append(avgAccLoss) # save the acc \n",
    "    \n",
    "    # WANDB\n",
    "    wandb.log({\n",
    "    # \"Examples\": example_images,\n",
    "    \"true_positives\": avg_tp, \n",
    "    \"true_negatives\": avg_tn, \n",
    "    \"false_negatives\": avg_fn,\n",
    "    \"false_positives\": avg_fp,\n",
    "    \"Train Loss\": avgTrainLoss,\n",
    "    \"Train Accuracy\": avgAccLoss,\n",
    "    \"IoU_train\":avgIOU})\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader, lossFunc, epoch):\n",
    "    \n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    # Save total train loss\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # metrics\n",
    "    accuracy_val = 0\n",
    "    iou_val = 0\n",
    "    f1score_val = 0\n",
    "     \n",
    "    # testing\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    \n",
    "    # switch off autograd\n",
    "    example_pred = []\n",
    "    example_gt = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # loop over the validation set\n",
    "        loop = tqdm(dataloader, leave=False)\n",
    "        for (x_val, y_val) in loop:\n",
    "            # send the input to the device\n",
    "            (x_val, y_val) = (x_val.to(config.DEVICE), y_val.to(config.DEVICE))\n",
    "            \n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred_val = model(x_val)\n",
    "            loss = lossFunc(pred_val, y_val)\n",
    "            totalValLoss += loss\n",
    "            \n",
    "            # metrics      \n",
    "            all_metrics = metrics.metrics(pred_val, y_val)\n",
    "            accuracy_val += all_metrics['acc']\n",
    "            iou_val += all_metrics['iou']\n",
    "            f1score_val += all_metrics['f1score']\n",
    "            \n",
    "            tp += all_metrics['false_preds'][0]\n",
    "            fp += all_metrics['false_preds'][1]\n",
    "            fn += all_metrics['false_preds'][2]\n",
    "            tn += all_metrics['false_preds'][3]\n",
    "            \n",
    "            # # Plotting Val \n",
    "            # if iter_  % 2 == 0: # plot every time iter is a multiple of 2\n",
    "            # utis.plot_comparison(x_val, pred_val, y_val)\n",
    "            # iter_ + 1\n",
    "            print(y_val[0].shape)\n",
    "            print(type(y_val[0]))\n",
    "            # WandB – Log images in your test dataset automatically, along with predicted and true labels by passing pytorch tensors with image data into wandb.Image\n",
    "            example_pred.append(wandb.Image(pred_val[0], caption=\"pred\"))\n",
    "            # example_gt.append(wandb.Image(y_val[0], caption=\"GT\"))\n",
    "            \n",
    "            # update tqdm\n",
    "            loop.set_description(f'Validation Epoch [{epoch}/{config.NUM_EPOCHS}]')\n",
    "            loop.set_postfix(loss_val=loss.item(), acc_val = all_metrics['acc'], iou_val=all_metrics['iou'])\n",
    "                        \n",
    "    # calculate the average VALIDATION loss PER EPOCH\n",
    "    avgValLoss = totalValLoss / len(dataloader)\n",
    "    avgAccLoss = accuracy_val / len(dataloader)\n",
    "    avgIOU = iou_val / len(dataloader)\n",
    "    avgF1score = f1score_val / len(dataloader)\n",
    "    \n",
    "    avg_tp = tp / len(dataloader)\n",
    "    avg_fp = fp / len(dataloader)\n",
    "    avg_fn = fn / len(dataloader)\n",
    "    avg_tn = tn / len(dataloader)\n",
    "\n",
    "    ## update VALIDATION history\n",
    "    validation_history[\"avg_val_loss\"].append(avgValLoss.cpu().detach().numpy()) # save the avg loss\n",
    "    validation_history[\"val_accuracy\"].append(avgAccLoss) # save the acc\n",
    "    \n",
    "    # WANDB\n",
    "    wandb.log({\n",
    "    \"Predictions\": example_pred,\n",
    "    # \"GT\": example_gt,\n",
    "    \"true_positives_val\": avg_tp, \n",
    "    \"true_negatives_val\": avg_tn, \n",
    "    \"false_negatives_val\": avg_fn,\n",
    "    \"false_positives_val\": avg_fp,\n",
    "    \"Val Accuracy\": avgAccLoss,\n",
    "    \"Val Loss\": avgValLoss,\n",
    "    \"IoU_val\": avgIOU})\n",
    "    \n",
    "    return validation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvibferreira\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/mastoc/projects/erasmus/dl_historical_images/master_thesis/utils/wandb/run-20220328_182049-3uja5p0o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vibferreira/master_thesis/runs/3uja5p0o\" target=\"_blank\">legendary-star-239</a></strong> to <a href=\"https://wandb.ai/vibferreira/master_thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network for 5 epochs, with a batch size of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_EPOCHS):\n\u001b[1;32m     33\u001b[0m     trained \u001b[38;5;241m=\u001b[39m train(unet, train_dataloader, opt, lossFunc, epoch\u001b[38;5;241m=\u001b[39me, scaler\u001b[38;5;241m=\u001b[39mscaler)\n\u001b[0;32m---> 34\u001b[0m     validated \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossFunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m best_accuracy : \u001b[38;5;66;03m# maybe add a minimum number of epochs as conditions\u001b[39;00m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, dataloader, lossFunc, epoch)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# WandB – Log images in your test dataset automatically, along with predicted and true labels by passing pytorch tensors with image data into wandb.Image\u001b[39;00m\n\u001b[1;32m     53\u001b[0m example_pred\u001b[38;5;241m.\u001b[39mappend(wandb\u001b[38;5;241m.\u001b[39mImage(pred_val[\u001b[38;5;241m0\u001b[39m], caption\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 54\u001b[0m example_gt\u001b[38;5;241m.\u001b[39mappend(\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# update tqdm\u001b[39;00m\n\u001b[1;32m     57\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/wandb/sdk/data_types.py:2111\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data_or_path, mode, caption, grouping, classes, boxes, masks)\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_from_path(data_or_path)\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_initialization_meta(grouping, caption, classes, boxes, masks)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/wandb/sdk/data_types.py:2226\u001b[0m, in \u001b[0;36mImage._initialize_from_data\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m   2224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m   2225\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m-> 2226\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mvis_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mfromarray(\n\u001b[1;32m   2228\u001b[0m         data\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   2229\u001b[0m     )\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/utils.py:87\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             norm_range(t, value_range)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[43mnorm_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/utils.py:81\u001b[0m, in \u001b[0;36mmake_grid.<locals>.norm_range\u001b[0;34m(t, value_range)\u001b[0m\n\u001b[1;32m     79\u001b[0m     norm_ip(t, value_range[\u001b[38;5;241m0\u001b[39m], value_range[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mnorm_ip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/utils.py:75\u001b[0m, in \u001b[0;36mmake_grid.<locals>.norm_ip\u001b[0;34m(img, low, high)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm_ip\u001b[39m(img, low, high):\n\u001b[1;32m     74\u001b[0m     img\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mlow, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mhigh)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(\u001b[38;5;28mmax\u001b[39m(high \u001b[38;5;241m-\u001b[39m low, \u001b[38;5;241m1e-5\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Byte"
     ]
    }
   ],
   "source": [
    "# WandB – Initialize a new run\n",
    "wandb.init(entity=\"vibferreira\", project=\"master_thesis\")\n",
    "\n",
    "# classes\n",
    "classes = ('no_vegetation', 'vegetation')\n",
    "\n",
    "# Initialize model\n",
    "unet = model.unet_model.to(config.DEVICE)\n",
    "\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "opt = optim.Adam(unet.parameters(), lr=config.LR)\n",
    "\n",
    "# initialize a dictionary to store TRAINING history (keep track on training)\n",
    "training_history = {\"avg_train_loss\": [], \"train_accuracy\": [], \"IoU\":[],\"f1score\":[], \"avgDice\":[]}\n",
    "\n",
    "# # initialize a dictionary to store VALIDATION history (keep track on VALIDATION)\n",
    "validation_history = {\"avg_val_loss\": [], \"val_accuracy\": [], \"IoU_val\":[], \"f1score_val\":[]}\n",
    "\n",
    "# Using log=\"all\" log histograms of parameter values in addition to gradients\n",
    "wandb.watch(unet, log=\"all\")\n",
    "\n",
    "# Autocasting \n",
    "scaler = GradScaler()\n",
    "\n",
    "# initialize best accuracy\n",
    "best_accuracy = 0.0\n",
    "print(f'''Training the network for {config.NUM_EPOCHS} epochs, with a batch size of {config.BATCH_SIZE}''') # try with logger\n",
    "\n",
    "# loop = tqdm(range(config.NUM_EPOCHS))\n",
    "iter_ = 0\n",
    "for e in range(config.NUM_EPOCHS):\n",
    "    trained = train(unet, train_dataloader, opt, lossFunc, epoch=e, scaler=scaler)\n",
    "    validated = validation(unet, val_dataloader, lossFunc, epoch=e)\n",
    "    \n",
    "    # Save best model\n",
    "    if validated['val_accuracy'][-1] > best_accuracy : # maybe add a minimum number of epochs as conditions\n",
    "        utis.save_best_model(unet, BEST_MODEL, validated, e)\n",
    "        best_accuracy = validation_history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_predictions_as_imgs():\n",
    "#     # test dataloader\n",
    "#     pass\n",
    "\n",
    "# # load the model \n",
    "# # Initialize our model\n",
    "# unet = model.unet_model.to(config.DEVICE)\n",
    "# path_model = '../best_model/best_model_epoch_4_acc_0.985_iou_0.003.pth' \n",
    "# unet.load_state_dict(torch.load(path_model))\n",
    "# folder = '../saved_images' \n",
    "\n",
    "# y_hat_test = []\n",
    "# y_true_test = []\n",
    "\n",
    "# # switch off autograd\n",
    "# with torch.no_grad():\n",
    "#     # set the model in evaluation mode\n",
    "#     unet.eval()\n",
    "\n",
    "#     # loop over the validation set\n",
    "#     loop = tqdm(test_dataloader, leave=False)\n",
    "\n",
    "#     for idx, (x_test, y_test) in enumerate(loop):\n",
    "#         # send the input to the device\n",
    "#         (x_test, y_test)  = (x_test.to(config.DEVICE), y_test.to(config.DEVICE))\n",
    "\n",
    "#         # Predictions\n",
    "#         pred_test = unet(x_test)\n",
    "\n",
    "#         # Assign appropriate class # ASK SEBASTIAN\n",
    "#         pred_test = pred_test.sigmoid()\n",
    "#         pred_test = (pred_test > 0.5).float()\n",
    "\n",
    "#         # Storing predictions and true labels \n",
    "#         y_hat_test.append(pred_test)\n",
    "#         y_true_test.append(y_test)\n",
    "\n",
    "#         # metrics\n",
    "#         utis.plot_comparison(x_test, pred_test, y_test)\n",
    "\n",
    "#         # Save images\n",
    "#         # save_image(pred_test, f\"{folder}/pred_{idx}.png\") \n",
    "#         # save_image(y_test, f\"{folder}/y_true_{idx}.png\")\n",
    "\n",
    "# # # Stack and flatten for confusion matrix # GETTING SIZE ERROR AT THE MOMENT\n",
    "# # torch.stack(y_hat_test).flatten()\n",
    "# # torch.stack(y_true_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_test[0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "# ax[0].plot(training_history['avg_train_loss'], label= 'train')\n",
    "# ax[0].plot(validation_history['avg_val_loss'], label='val')\n",
    "# ax[0].set_title('Loss')\n",
    "# ax[1].plot(training_history['train_accuracy'], label= 'train')\n",
    "# ax[1].plot(validation_history['val_accuracy'], label='val')\n",
    "# ax[1].set_title('Validation')\n",
    "# ax[1].legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5176260fbc2cf1b78dfea4cc2edeb44537a6fdfe3203945a451ce281e8b97dc2"
  },
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
